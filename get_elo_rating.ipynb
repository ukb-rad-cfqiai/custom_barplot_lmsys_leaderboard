{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is modified from a notebook published from Universtiy Berkley taken from\n",
    "# https://colab.research.google.com/drive/1C2tQ-1j2Nm-NmtAx-Lo2wFTTod_s9jfe#scrollTo=mSizG3Pzglte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyl5Vil7HRzd"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we present data analysis on Chatbot Arena data collected from https://arena.lmsys.org between April 24, 2023 to Apr 9, 2024.\n",
    "\n",
    "We explain different Elo calculation methods (online Elo and MLE Elo, also known as Bradley-Terry model) for model ranking.\n",
    "\n",
    "To view the latest leaderboard, see https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5H_wlbqGwCJ",
    "outputId": "e2ffab80-9171-4c6e-e2c0-aadbbf9002c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaleido\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json, math, gdown, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "%pip install kaleido\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZ0G_G-sHwm3"
   },
   "source": [
    "# Obtaining and Cleaning the Tournament Data\n",
    "We are hosting the initial tournament results as a JSON file on Google Drive. We use the `gdown` function to download the data. The data contains all the battels and voting results collected for ranking models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/arena_external_data/public/clean_battle_20240410.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('local_file_name.json', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# load the JSON data from the local file\n",
    "with open('local_file_name.json', 'r') as file:\n",
    "    battles = pd.read_json(file).sort_values(ascending=True, by=[\"tstamp\"])\n",
    "    \n",
    "battles = battles[battles[\"anony\"] == True]\n",
    "print(len(battles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_PYA7oVyaHO"
   },
   "source": [
    "#Elo Ratings\n",
    "\n",
    "The [Elo rating system ](https://en.wikipedia.org/wiki/Elo_rating_system)is a method for calculating the relative skill levels of players, which has been widely adopted in chess and other competitive games. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them.\n",
    "In this section, we present different methods for calculating Elo ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbTdhkLQp113"
   },
   "source": [
    "\n",
    "### Maximum Likelihood Estimation for Elo Ratings (aka [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model))\n",
    "\n",
    "In the context of LLM evaluation, models can be assumed to be static. In this case, we can directly fit the ratings by maximum likelihood estimation method (aka Bradley-Terry model), which produce significantly stable ratings. Here we provide an implementation with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSizG3Pzglte"
   },
   "outputs": [],
   "source": [
    "def compute_mle_elo(df, SCALE=400, BASE=10, INIT_RATING=1000):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    models = pd.concat([df[\"model_a\"], df[\"model_b\"]]).unique()\n",
    "    models = pd.Series(np.arange(len(models)), index=models)\n",
    "\n",
    "    # duplicate battles\n",
    "    df = pd.concat([df, df], ignore_index=True)\n",
    "    p = len(models.index)\n",
    "    n = df.shape[0]\n",
    "\n",
    "    X = np.zeros([n, p])\n",
    "    X[np.arange(n), models[df[\"model_a\"]]] = +math.log(BASE)\n",
    "    X[np.arange(n), models[df[\"model_b\"]]] = -math.log(BASE)\n",
    "\n",
    "    # one A win => two A win\n",
    "    Y = np.zeros(n)\n",
    "    Y[df[\"winner\"] == \"model_a\"] = 1.0\n",
    "\n",
    "    # one tie => one A win + one B win\n",
    "    # find tie + tie (both bad) index\n",
    "    tie_idx = (df[\"winner\"] == \"tie\") | (df[\"winner\"] == \"tie (bothbad)\")\n",
    "    tie_idx[len(tie_idx)//2:] = False\n",
    "    Y[tie_idx] = 1.0\n",
    "\n",
    "    lr = LogisticRegression(fit_intercept=False, penalty=None, tol=1e-8)\n",
    "    lr.fit(X,Y)\n",
    "\n",
    "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
    "\n",
    "    # set anchor as mixtral = 1114\n",
    "    if \"mixtral-8x7b-instruct-v0.1\" in models.index:\n",
    "        elo_scores += 1114 - elo_scores[models[\"mixtral-8x7b-instruct-v0.1\"]]\n",
    "    return pd.Series(elo_scores, index = models.index).sort_values(ascending=False)\n",
    "\n",
    "mle_elo_ratings = compute_mle_elo(battles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_bar_elo_rating(ratings):\n",
    "    df = pd.DataFrame([\n",
    "        [n, ratings[n]] for n in ratings.keys()\n",
    "    ], columns=[\"Model\", \"Elo rating\"]).sort_values(\"Elo rating\", ascending=False).reset_index(drop=True)\n",
    "    df.index = df.index + 1\n",
    "\n",
    "    # Filter out rows with \"gpt-4\" substring except the one with the highest \"Elo rating\"\n",
    "    models_to_filter_for_best = [ 'gpt-4', 'gpt-3.5', 'gemini-pro', 'starling-lm-7b']\n",
    "    for model in models_to_filter_for_best:\n",
    "      model_rows = df[df['Model'].str.contains(model)]\n",
    "      if not model_rows.empty:\n",
    "          max_rating = model_rows['Elo rating'].max()\n",
    "          df = df[~(df['Model'].str.contains(model) & (df['Elo rating'] < max_rating))]\n",
    "\n",
    "    # MODIFIED\n",
    "    # openchat and starling are versions of mistral-7b so only keep best\n",
    "    # wizard-LM-70b and tulu-2-dpo-70b are versions of llama-2-70b  so only keep best\n",
    "    models_custom_remove = ['openchat-3.5', 'openchat-3.5', 'tulu-2-dpo-70b']\n",
    "    for model in models_custom_remove:\n",
    "      df = df[~df['Model'].str.contains(model)]\n",
    "    print(df['Model'])\n",
    "\n",
    "    # MODIFIED\n",
    "    # only top 25 and make bar plot\n",
    "    df = df.head(25)\n",
    "    fig = px.bar(df, x='Model', y='Elo rating')\n",
    "    fig.update_yaxes(range=[800, 1300])\n",
    "    fig.update_layout(\n",
    "        font_family=\"Calibri\",\n",
    "        title_font_family=\"Calibri\",\n",
    "        font_size=12,\n",
    "        title_font_size=18\n",
    "    )\n",
    "\n",
    "    return fig, df\n",
    "\n",
    "fig, df = custom_bar_elo_rating(mle_elo_ratings)\n",
    "import matplotlib.pyplot as plt\n",
    "fig.write_image('elo.png', scale=2)\n",
    "from google.colab import files\n",
    "files.download('elo.png')\n",
    "\n",
    "df.to_excel('elo.xlsx')\n",
    "files.download('elo.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
